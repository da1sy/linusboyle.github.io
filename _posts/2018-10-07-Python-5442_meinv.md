---
title: Python爬虫之爬取站内所有图片
date: 2018-10-07
tags: Python
layut: post
---

>目标是 **[http://www.5442.com/meinv/](http://www.5442.com/meinv/ "MV")**

### 捧上狗屎代码
```python
#-*- coding:utf-8 -*-
import re
import urllib
import urllib2
import os
import chardet
import sys
'''
def get_html(url):#获取网页内容
    try:
        request = urllib2.Request(url,headers=ua_headers)
        response = urllib2.urlopen(request)
        html = response.read()
        return html
    except:
        print "获取内容失败"
'''
def get_html(url):#获取网页内容
    try:
        request = urllib2.Request(url,headers=ua_headers)
        data = urllib2.urlopen(request).read()
        typeEncode = sys.getfilesystemencoding()
        infoencode = chardet.detect(data).get('encoding','gb2312')
        html = data.decode(infoencode,'ignore').encode(typeEncode)
        return html
    except:
        print "获取内容失败"

ua_headers={    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.114 Safari/537.36',
    'Cookie': 'AspxAutoDetectCookieSupport=1'
}

url = "http://www.5442.com/meinv/"
    ########################################
#先获取meinv页面内的所有tag链接。然后利用tag的名字创建相应的目录。
    ########################################

tag_code = get_html(url)
tag_egrep = r'href="(.*).*" class="'
tag_url_list = re.findall(tag_egrep, tag_code)
print
print "[V]已成功爬去meinv页面内所有tag分类的链接"
print tag_url_list##打印meinv页面的所有链接
for tag_url in tag_url_list:
    ########################################
    try:
        tag_dir_name = tag_url[24:-5]
        #tag_mkpath = "C:\\Users\\Administrator\\Desktop\\Python-learn\\Photos\\" + tag_dir_name
        tag_mkpath = "Photos/" + tag_dir_name
        tag_mkdir = os.path.exists(tag_mkpath)
        print
        print "...已成功匹配到该Tag的名称:" + tag_dir_name
        if not tag_mkdir:
            os.makedirs(tag_mkpath)
            print "...创建%s目录成功----"%tag_dir_name
        else:
            print "...已有此%s目录----"%tag_dir_name
    except:
        print "...[X]获取%s链接失败或创建%s文件夹失败[X]"%tag_dir_name
    tz_code = get_html(tag_url)
    tz_url_egrep = r'href="(.*).*" target="_blank" title="'
    tz_url_list = re.findall(tz_url_egrep,tz_code) 
    '''
    print "......已成功爬取该%s内的帖子链接"%tag_dir_name
    print "......[所有%s帖子的链接]"%tag_dir_name
    print tz_url_list
    '''
    print tz_url_list
    for tz_url in tz_url_list:
        print ".........当前帖子链接---"+tz_url
        xx = 0
        while True : 
             #tz_name_egrep = r'_blank" title="(.*?)">'
            tz_name_egrep = r"<img alt='(.*?)' src"
            tz_name_list = re.findall(tz_name_egrep, tz_code)
            x = 0
            if xx == 0:
                tz_HQ_url = tz_url
            else:
                tz_hz_url = tz_url[-5:]
                tz_qz_url = tz_url[:-5]+"_"
                tz_HQ_url = tz_qz_url + str(xx) + tz_hz_url
                img_code = get_html(tz_HQ_url)
                img_url_egrep = r"src='(.*).*' alt=''"
                img_url_list = re.findall(img_url_egrep,img_code)
            name = dict(zip(tz_name_list,tz_HQ_url))
            print tz_name_list
            print tz_HQ_url
```
###捧上低帧GIF
![1](http://da1sy.github.io/assets/images/10-Yue/5442_meinv.gif)